# ZenoZero ‚Äî Topology-Aware Meta-Controlled Reversi Engine

**Version:** ZenoZero 1.0.0  
**Game:** 8√ó8 Reversi (Othello)  
**Architecture:** Topology-Aware Meta-Controlled MCTS + Neural Network

---

## What Is ZenoZero?

ZenoZero is **not** vanilla AlphaZero with tweaks.

Traditional AlphaZero-style systems treat every position with the same fixed
search budget and the same fixed exploration coefficient. ZenoZero replaces
both with a **topology-aware meta-controller** that reads the geometry of the
MCTS tree in real time and adapts every parameter accordingly.

The core insight is that the MCTS tree itself is a signal. When visits have
collapsed onto one branch (low entropy), when one move dominates (high gap),
and when all children agree on value (low variance) ‚Äî the position is
structurally clear and heuristic guidance is trustworthy. When the tree is
diffuse and children disagree, the neural network should be trusted more and
the search should run longer.

ZenoZero formalises this intuition into a layered, ablation-ready system
called the **ZenoZero Architecture**.

---

## File Structure

```
ZenoZero_reversi/
‚îÇ
‚îú‚îÄ‚îÄ reversi_phase5_topology_core.py          # Layer 0 ‚Äî game engine + NN + Numba kernels
‚îú‚îÄ‚îÄ reversi_phase5_topology_layers.py        # Layers 1‚Äì8 ‚Äî full topology-aware MCTS
‚îú‚îÄ‚îÄ reversi_phase5_baseline.py               # Pure MCTS baseline (fixed 800 budget)
‚îú‚îÄ‚îÄ reversi_phase5_dynamic_threshold_recalibrator.py  # Auto-calibrates early-stop thresholds
‚îú‚îÄ‚îÄ reversi_phase5_training.py               # Self-play + training loop (multi-worker)
‚îú‚îÄ‚îÄ reversi_phase5_benchmark.py              # Benchmarking + ablation matrix
‚îî‚îÄ‚îÄ README.md                                # This file
```

---

## The ZenoZero Architecture ‚Äî 8 Layers

The system is split into three control planes:

| Plane | Layers | Responsibility |
|---|---|---|
| **Search Plane** | 0 | Neural network, tactical solver, MCTS mechanics |
| **Topology Plane** | 1, 6 | Sensing tree geometry; early-stop gating |
| **Meta-Control Plane** | 2, 3, 4, 5, 7, 8 | Heuristic injection, Œª controller, exploration, budget, logging |

---

### Layer 0 ‚Äî Baseline (`reversi_phase5_topology_core.py`)

The foundation everything else plugs into.

**Game Engine ‚Äî `ReversiGame`**
- Full 8√ó8 Reversi rules: flipping, passing, consecutive-pass termination
- `make_move`, `get_legal_moves`, `copy`, board display
- All hot-path methods delegate to Numba kernels (see below)

**Numba Kernels** ‚Äî compiled at import time, `cache=True`
- `_nb_is_legal(board, row, col, player)` ‚Äî single legality test
- `_nb_compute_legal_moves(board, player)` ‚Äî full legal move scan
- `_nb_get_flips(board, row, col, player)` ‚Äî flip calculation for `make_move`
- `_nb_ucb_select(q, priors, visits, parent_n, c_puct, h_astars, Œª, use_h)` ‚Äî **exported**, used by all four MCTS files; replaces the Python loop over children that would otherwise be called millions of times per training run

`_nb_compute_legal_moves` and `_nb_is_legal` are the highest-frequency calls
in the entire system ‚Äî they fire on every node expansion and every heuristic
evaluation. `_nb_ucb_select` is the second hottest path.

**MCTS Node ‚Äî `MCTSNode`**
- Dataclass: `visit_count`, `value_sum`, `prior`, `children`, `untried_moves`
- `h_astar: float = 0.0` ‚Äî cached heuristic score, **set once at expansion,
  read in every subsequent selection**. Eliminates the O(board¬≤) heuristic
  call that would otherwise fire inside the hot simulation loop.

**Tactical Solver ‚Äî `TacticalSolver`**
- Layer 0 shortcut that bypasses MCTS entirely for obvious moves
- Priority 1: Corner available ‚Üí take it (instant, massive positional value)
- Priority 2: Only one legal move ‚Üí play it (no decision needed)
- Priority 3: Forced pass ‚Üí return `None` move
- Corner captures are the Reversi equivalent of Gomoku's "immediate win" ‚Äî
  structural value is so dominant that MCTS budget is wasted deliberating

**Pattern Heuristic ‚Äî `PatternHeuristic`**
Four sub-scores, all normalised to `[-1, 1]`, combined with game-phase weights:

| Sub-score | Early weight | Late weight | Description |
|---|---|---|---|
| Positional | 0.30 | 0.10 | Corner-heavy static weight table |
| Mobility | 0.35 | 0.10 | Legal move count ratio |
| Stability | 0.30 | 0.30 | Edges + corners held (approximation) |
| Parity | 0.05 | 0.50 | Disc count ratio |

Weights shift at move 50 ‚Äî parity matters most in endgame, mobility matters
most mid-game. The mobility calculation calls `_nb_compute_legal_moves`
internally so it benefits from the Numba speedup.

**Neural Network ‚Äî `CompactReversiNet`**

```
Input:  4 channels √ó 8√ó8
  ch0  my pieces
  ch1  opponent pieces
  ch2  legal move mask        ‚Üê lets network learn to suppress illegal moves
  ch3  player indicator (+1 or -1)

Tower:  4 √ó Conv2d(128, 3√ó3) + BatchNorm + ReLU

Policy head:  Conv2d(2,1√ó1) ‚Üí Linear ‚Üí 65 raw logits
              (64 squares + index 64 = pass action)

Value head:   Conv2d(1,1√ó1) ‚Üí Linear(64) ‚Üí Linear(1) ‚Üí tanh
              scalar ‚àà [-1, 1]
```

`forward()` returns **raw logits** ‚Äî used directly with cross-entropy loss
during training. `predict()` masks illegal moves before softmax so MCTS never
allocates prior probability to illegal actions.

---

### Layer 1 ‚Äî Visit Metrics / Tree Topology Sensors (`TreeMetrics`)

Observables of tree geometry. **Read-only ‚Äî does not change behaviour.**

```
H_v   = normalised visit entropy     ‚àà [0, 1]
        1 = visits distributed uniformly across children
        0 = all visits collapsed onto one child

G     = dominance gap                ‚àà [0, 1]
        (visits_top1 - visits_top2) / total_visits
        High G means one move is clearly dominant

Var_Q = value variance               ‚àà [0, ‚àû)
        variance of mean Q-values across visited children
        Low = children agree; High = contested position
```

These three scalars form the input vector to the meta-control system.
They are recomputed every 50 simulations (not every simulation) to
avoid making metric computation itself a bottleneck.

---

### Layer 2 ‚Äî Weak Heuristic Injection (A\*-Inspired UCB Bias)

Modifies the child selection score:

```
Old (standard PUCT):
  score = Q + c_puct ¬∑ P ¬∑ ‚àöN / (1 + n)

New (ZenoZero):
  score = Q + c_puct ¬∑ P ¬∑ ‚àöN / (1 + n)  +  Œª ¬∑ h_astar
```

Where:
- `h_astar` = `PatternHeuristic.evaluate()` result, clipped to `[-1, 1]`,
  **computed once at expansion and cached on the node** as `child.h_astar`
- `Œª` = dynamic weight from Layer 4

The heuristic is intentionally weak ‚Äî it nudges the energy landscape of the
tree rather than replacing search. This preserves generalisation while
leveraging structural knowledge.

The entire scoring computation runs inside `_nb_ucb_select` (the Numba kernel)
‚Äî no Python loop over children during selection.

---

### Layer 3 ‚Äî Soft Pruning via Prior Scaling

At expansion time, the child's prior is scaled:

```python
child.prior *= exp(-0.5 * penalty)
```

Where `penalty` is derived from the already-computed `h_astar`:

| h_astar | Penalty | Effect |
|---|---|---|
| ‚â• 0.0 | 0.0 | No change |
| [-0.5, 0.0) | 0.5 | Prior halved approx. (√ó0.78) |
| < -0.5 | 1.0 | Prior significantly reduced (√ó0.61) |

This is A\*-style "inadmissible but helpful" guidance. Hard pruning would
destroy tree structure; soft pruning discourages bad moves while keeping
them explorable.

---

### Layer 4 ‚Äî Dynamic Œª Controller (Meta-Control)

```
Œª = 0.4 ¬∑ (1 - H_v)  +  0.4 ¬∑ G  +  0.2 ¬∑ (1 - clamp(Var_Q, 0, 1))
Œª ‚àà [0, 1]
```

| Condition | Effect on Œª | Meaning |
|---|---|---|
| Low entropy (H_v ‚Üí 0) | Œª ‚Üë | Tree concentrated ‚Üí trust heuristic |
| High gap (G ‚Üí 1) | Œª ‚Üë | Dominant move exists ‚Üí trust structure |
| Low variance (Var_Q ‚Üí 0) | Œª ‚Üë | Children agree ‚Üí stable evaluation |
| All reversed | Œª ‚Üì | Uncertain position ‚Üí trust neural network |

Œª is the **global meta-control signal**. It simultaneously controls:
- Heuristic injection weight (Layer 2)
- Search budget allocation (Layer 7)

The deterministic formula is the current implementation. **Phase 5.5** (future)
replaces it with a small MLP trained offline from the Layer 8 logs:

```python
meta_controller = nn.Sequential(
    nn.Linear(3, 32),   # [H_v, G, Var_Q] ‚Üí hidden
    nn.ReLU(),
    nn.Linear(32, 1),   # hidden ‚Üí Œª
    nn.Sigmoid()
)
```

---

### Layer 5 ‚Äî Entropy-Aware Exploration

```
c_puct = c‚ÇÄ ¬∑ (1 + H_v)
```

Default `c‚ÇÄ = 1.414`. Range roughly `[1.414, 2.828]`.

- High entropy (diffuse tree) ‚Üí `c_puct` high ‚Üí explore more
- Low entropy (concentrated) ‚Üí `c_puct` low ‚Üí exploit the dominant branch

This is **independent** of Œª ‚Äî two separate knobs for exploration and
heuristic trust.

---

### Layer 6 ‚Äî Spectral Gap Early Stop

Halts simulation early when all three topology signals agree:

```python
if H_v < H_v_thresh and G > G_thresh and Var_Q < Var_Q_thresh:
    stop_search()
```

The thresholds are **not hardcoded** ‚Äî they are calibrated by
`DynamicRecalibrator` against the current model's actual probe distributions
(see Calibrator section). Default values before first calibration:
`H_v < 0.20`, `G > 0.50`, `Var_Q < 0.02`.

Only activates after a minimum of 100 simulations ‚Äî prevents premature
stopping before the tree has enough information.

---

### Layer 7 ‚Äî Budget Control via Œª

```
budget = base ¬∑ phase_mult ¬∑ Œª_mult
```

| Œª range | Œª_mult | Interpretation |
|---|---|---|
| > 0.7 | 0.7 | Structure clear ‚Üí save compute |
| 0.3‚Äì0.7 | 1.0 | Normal |
| < 0.3 | 1.3 | Uncertain ‚Üí invest more |

Game phase multipliers:

| Phase | Pieces on board | Multiplier |
|---|---|---|
| Opening | < 16 | 0.7 |
| Midgame | 16‚Äì48 | 1.2 |
| Endgame | > 48 | 0.8 |

Budget is determined via a **probe-then-search** pattern: the first 100
simulations run on the real root (not a throwaway copy), Œª is estimated from
those results, the full budget is computed, and the remaining `budget - 100`
simulations complete the search.

---

### Layer 8 ‚Äî Comprehensive Logging

Every non-tactical move logs to CSV:

```
move_num, player, H_v, G, Var_Q, lambda_h, c_puct,
budget, tactical, board_density, phase, win_outcome
```

This dataset is the future training signal for the Phase 5.5 learned Œª
controller ‚Äî `[H_v, G, Var_Q]` ‚Üí `Œª_optimal` derived from `win_outcome`.

---

## Dynamic Threshold Recalibrator

`DynamicRecalibrator` keeps Layer 6's early-stop thresholds aligned with the
current model's strength. As the network improves through training iterations,
visit distributions tighten ‚Äî a stronger model collapses visits faster, so
fixed thresholds become stale.

**Calibration process:**
1. Collect N board positions from random self-play (default N=300)
2. Skip positions where `TacticalSolver` fires (they wouldn't use early stop)
3. Run `probe_budget` simulations on each position
4. Compute `H_v`, `G`, `Var_Q` distributions
5. Set thresholds at percentiles (25th for H_v and Var_Q, 75th for G)
6. This targets ~25% early-stop rate ‚Äî enough to save compute without
   cutting off genuinely contested positions

**Recalibration triggers:**
- **Periodic** ‚Äî every `recal_interval` training iterations (default: 5)
- **Drift detection** ‚Äî mini-probe (50 positions) checks if any metric mean
  has shifted by more than `drift_threshold` standard deviations vs last
  calibration
- **Manual** ‚Äî `recalibrator.recalibrate_now()`

Calibration results are saved to `calibrations/calibration_iter{N:04d}.json`
and `calibrations/calibration_latest.json` for easy loading.

---

## Pure MCTS Baseline

`PureMCTS` is the **control** for all experiments.

| Property | Baseline | ZenoZero |
|---|---|---|
| Budget | Fixed 800 | Œª-adaptive (150‚Äì800) |
| c_puct | Fixed 1.414 | Dynamic f(H_v) |
| Heuristic | None | Œª-weighted A\* injection |
| Pruning | None | Soft exp(-Œ≤¬∑penalty) |
| Early stop | Never | Topology-gated |
| Meta-control | None | Œª = f(H_v, G, Var_Q) |
| Architecture | Identical NN | Identical NN |

The network architecture is **identical** in both systems ‚Äî comparisons
isolate the topology layers, not model capacity.

---

## Training

AlphaZero-style self-play + supervised learning from visit distributions.

**Self-play workers:** `cpu_count() // 2 - 1` parallel processes  
Each worker runs complete games and sends `SelfPlayRecord` batches to the
main process via `mp.Queue`. Workers receive updated network weights every
`weight_push_interval` games (default: 5).

**Temperature schedule:**
- Moves 0‚Äì29: `temperature = 1.0` + Dirichlet noise `(Œ±=0.3, Œµ=0.25)`
- Moves 30+: `temperature = 0.0` (greedy)

**Training targets:**
- `policy_target`: MCTS visit distribution (shape `[65]`) ‚Äî cross-entropy loss
- `value_target`: game outcome `{-1, 0, 1}` from player's perspective ‚Äî MSE loss

**Loss:**
```
L = CE(policy_logits, visit_distribution) + MSE(value_pred, outcome)
```

**Replay buffer:** circular deque, default `80,000` positions.
Training begins once buffer exceeds `min_buffer` (default `2,000`).

**`SelfPlayRecord` training hook:**
```python
# During self-play:
move, policy, stats, record = mcts.search(game, return_record=True)

# After game ends ‚Äî annotate all records with outcome:
for r in records:
    r.set_outcome(game.winner)

# Push to buffer:
buffer.push(records)
```

**Checkpoint format:**
```
{
  model_state_dict,
  optimizer_state_dict,
  iteration,
  config,
  thresholds,     # calibrated early-stop thresholds at save time
  log
}
```

---

## Benchmarking & Ablation

`reversi_phase5_benchmark.py` runs ZenoZero against the baseline and reports:

- Win / draw / loss rates (from ZenoZero's perspective)
- Average simulations per game (compute cost)
- Compute savings % vs fixed-800 baseline
- Tactical hit rate (fraction of moves resolved instantly)
- Average Œª and H_v (topology signal health check)
- Binomial significance test (p-value vs 50% win rate)

**Ablation matrix** (`--ablation` flag) tests each layer individually:

```
Baseline (no topology)
+L2       heuristic injection, fixed Œª
+L2+L4    + dynamic Œª controller
+L2+L4+L5 + entropy-aware exploration
+L2+L4+L5+L6  + early stop
+All layers   full ZenoZero system
```

---

## Quick Start

```bash
# 1. Install dependencies
pip install torch numpy numba scipy

# 2. Verify all components
python3 reversi_phase5_topology_core.py
python3 reversi_phase5_topology_layers.py
python3 reversi_phase5_baseline.py
python3 reversi_phase5_dynamic_threshold_recalibrator.py

# 3. Start training (small run to verify pipeline)
python3 reversi_phase5_training.py --iterations 3 --games-per-iter 10 --min-buffer 100

# 4. Full training run
python3 reversi_phase5_training.py --iterations 50 --games-per-iter 40

# 5. Benchmark trained model
python3 reversi_phase5_benchmark.py --checkpoint checkpoints/iter_0050.pt

# 6. Full ablation study
python3 reversi_phase5_benchmark.py --checkpoint checkpoints/iter_0050.pt --ablation --games 200
```

---

## Design Philosophy

**ZenoZero is not:**
- AlphaZero with a higher simulation budget
- A hand-crafted rule-based system
- Gomoku ported to Reversi

**ZenoZero is:**
- A topology-aware search system that treats the MCTS tree as a live sensor
- A meta-controlled engine where Œª = f(tree_geometry) at every move
- A foundation for learned meta-controllers (Phase 5.5 MLP)
- Ablation-ready by design ‚Äî every layer has an enable/disable toggle

The key contribution is the **dynamic Œª controller**. Fixed-Œµ heuristic
injection (as in Phase 4) treats every position identically. ZenoZero
recognises that the same heuristic is very trustworthy in some positions and
actively misleading in others ‚Äî and uses the tree's own geometry to determine
which regime it is in.

---

## Roadmap

| Version | Status | Description |
|---|---|---|
| **ZenoZero 1.0.0** | ‚úÖ Current | Deterministic Œª, full 8-layer system, Numba kernels |
| ZenoZero 1.1.0 | üî≤ Planned | Learned Œª MLP trained from Layer 8 logs |
| ZenoZero 1.2.0 | üî≤ Planned | Batched network evaluation for parallel MCTS |
| ZenoZero 1.3.0 | üî≤ Planned | Residual tower (replace compact conv with ResNet blocks) |
| ZenoZero 2.0.0 | üî≤ Future | Generalise architecture to arbitrary two-player zero-sum games |

---

## Dependency Notes

- **Python** 3.10+
- **PyTorch** 2.0+ (CPU or CUDA)
- **Numba** 0.57+ (`cache=True` requires write access to `__pycache__`)
- **NumPy** 1.24+
- **SciPy** 1.10+ (binomial test in benchmark script)
- No Phase 4 / Gomoku dependencies ‚Äî ZenoZero 1.0.0 is fully self-contained

---

*ZenoZero 1.0.0 ‚Äî A lightweight, tree-topology-based approximation to rational metareasoning in MCTS.*